traefik:
  image:
    #registry: public.ecr.aws/docker/library
    tag:

  rbac:
    enabled: true

  ingressRoute:
    healthcheck:
      enabled: true
      entryPoints:
        - web
        - websecure
  # Enable dashboard - don't keep it publicly open on production environments without appropriate protection (fw, security groups, auth, ip filtering, etc.)
    dashboard:
      enabled: true

  ports:
    traefik:
      expose:
        default: true # -- You SHOULD NOT expose the traefik port on production deployments.
        internal: true
    web:
      expose:
        default: true
        internal: true
    websecure:
      expose:
        default: true
        internal: true
      tls: {}

  # Enable access logs
  logs:
    general:
      # Format is required due to a bug in the helm chart for the specific version K3s is using at the time being (2023-09).
      format: common
      # Set the debug level to debug temporarily to troubleshoot
      #level: DEBUG
    access:
      enabled: true

  service:
    #externalTrafficPolicy: Local
    annotations:
      # On AWS, create a Network load balancer with IP target groups
      service.beta.kubernetes.io/aws-load-balancer-type: external
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
      service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
      service.beta.kubernetes.io/aws-load-balancer-attributes: load_balancing.cross_zone.enabled=true
      service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true
      service.beta.kubernetes.io/aws-load-balancer-name: "k8s-traefik"

      # You must specify three elastic IPs if you are using 3 availability zones and 3 subnets
      #service.beta.kubernetes.io/aws-load-balancer-eip-allocations: eipalloc-xxx, eipalloc-yyy, eipalloc-zzz

    additionalServices:
      internal:
        # Enable if you want a secondary internal service - for example an Internal NLB on AWS.
        enabled: false
        type: LoadBalancer
        single: true
        #externalTrafficPolicy: Local # Avoids an extra kube-proxy hop/SNAT, helpful for preserving client IP. But won't load balance across nodes.
        annotations:
          # On AWS, create an Internal Network Load Balancer with IP target groups
          service.beta.kubernetes.io/aws-load-balancer-type: external
          service.beta.kubernetes.io/aws-load-balancer-scheme: internal
          service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
          service.beta.kubernetes.io/aws-load-balancer-attributes: load_balancing.cross_zone.enabled=true
          # NAT loopback (hairpinning) is not supported when client ip preserving is enabled. If preserving client ip is needed, consider using proxy protocol v2.
          # https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html#loopback-timeout
          service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=false
          service.beta.kubernetes.io/aws-load-balancer-name: "k8s-traefik-internal"

  persistence:
    enabled: false

  providers:
    kubernetesCRD:
      # Allow the IngressRoute to create an empty servers load balancer if the targeted Kubernetes service has no endpoints available
      allowEmptyServices: true
      # Allow the IngressRoute to reference an externalName Service
      allowExternalNameServices: false
      # Allow the IngressRoute to reference a Service in a different namespace
      allowCrossNamespace: false

  ## Use an existing tls cert and set as default, for example when you use cert-manager for certificate management
  #tlsStore:
  #  default:
  #    defaultCertificate:
  #      secretName: wildcard-example-com-tls

  #
  # High Availability Configuration below
  #

  deployment:
    replicas: 3
    labels:
      tags.datadoghq.com/service: traefik
      tags.datadoghq.com/env: production
    podLabels:
      tags.datadoghq.com/service: traefik
      tags.datadoghq.com/env: production

  priorityClassName: "system-cluster-critical"

  topologySpreadConstraints:
    # This example topologySpreadConstraints forces the scheduler to put traefik pods on nodes where no other traefik pods are scheduled.
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app.kubernetes.io/instance: '{{ .Release.Name }}-{{ .Release.Namespace }}'
      matchLabelKeys:
        - pod-template-hash
    - maxSkew: 2
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/instance: '{{ .Release.Name }}-{{ .Release.Namespace }}'
      matchLabelKeys:
        - pod-template-hash

  # Termination grace period should be set higher than the "grace period" of the LB you are using
  terminationGracePeriodSeconds: 60

  # Recycle pods one by one
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  # Metrics
  metrics:
    otlp:
      enabled: false
      http:
        enabled: false
        endpoint: "http://datadog-agent.datadog.svc.cluster.local:4318/v1/metrics" # http://localhost:4318/v1/metrics
      grpc:
        enabled: false
        endpoint: "datadog-agent.datadog.svc.cluster.local:4317" # localhost:4317

  # Tracing
  tracing:
    otlp:
      enabled: false
      http:
        enabled: false
        endpoint: "http://datadog-agent.datadog.svc.cluster.local:4318/v1/traces" # http://localhost:4318/v1/traces
      grpc:
        enabled: false
        endpoint: "datadog-agent.datadog.svc.cluster.local:4317" # localhost:4317

  # Enable the experimental plugins
  experimental:
    plugins: {}
      # traefik-api-key-auth:
      #   moduleName: github.com/JimCronqvist/traefik-api-key-auth
      #   version: v0.0.2
