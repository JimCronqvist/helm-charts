
karpenter:
  logLevel: info
  podDisruptionBudget:
    name: karpenter
    maxUnavailable: 1
  controller:
    resources:
      requests:
          cpu: "1"
          memory: 1Gi
      limits:
          cpu: "1"
          memory: 1Gi
  additionalLabels:
    tags.datadoghq.com/service: karpenter
  podLabels:
    tags.datadoghq.com/service: karpenter
  podAnnotations:
    ad.datadoghq.com/controller.checks: |
      {
        "karpenter": {
          "init_config": {},
          "instances": [
            {
              "openmetrics_endpoint": "http://%%host%%:8000/metrics"
            }
          ]
        }
      }

  settings:
    # The EKS cluster name
    clusterName:
    # The public EKS Cluster https endpoint
    clusterEndpoint:
    # The Karpenter SQS queue name
    interruptionQueue:

    featureGates:
      drift: true
      spotToSpotConsolidation: true # Beta feature, use carefully.
  serviceAccount:
    # Override the name, as terraform will usually set the irsa to only allow 'karpenter:karpenter' in the trust policy.
    name: karpenter
    annotations:
      # Karpenter IAM role for service account (IRSA)
      eks.amazonaws.com/role-arn:

provisioners:
  - name: default
    nodePool: default
    nodeClass: al2
  - name: on-demand
    nodePool: onDemand
    nodePoolTaint: NoSchedule
    nodeClass: al2
  - name: unmanaged
    nodePool: unmanaged
    nodePoolTaint: NoSchedule
    nodeClass: al2
  - name: reserved-instances
    nodePool: reservedInstances
    nodeClass: al2

# Define a default role for all node classes
nodeClassDefaultRole:

nodeClass:
  al2:
    kubelet: {}
    detailedMonitoring: true

    amiSelectorTerms:
      - alias: al2@latest

    blockDeviceMappings:
      - deviceName: /dev/xvda
        ebs:
          volumeSize: 80Gi
          volumeType: gp3
          iops: 3000
          throughput: 125
          deleteOnTermination: true

# Create any number of node pools, with their own requirements.
# If you have multiple node pools that matches, you can also use a nodeSelector to assign pods to a specific node pool.
# But in most cases, weighting of node pool in combinations with limits is sufficient.
nodePool:
  default:
    # Karpenter works to actively reduce cluster cost by identifying when:
    # - Nodes can be removed as their workloads will run on other nodes in the cluster.
    # - Nodes can be replaced with cheaper variants due to a change in the workloads.
    consolidationPolicy: WhenEmptyOrUnderutilized

    # Wait a number of seconds after a node has no provisioned pods running before Karpenter requests to delete the node.
    # Ignored if consolidationPolicy is set.
    consolidateAfter: 2m

    # Define a lifetime on the provisioned nodes, to automatically force recreation with newer up-to-date AMIs with
    # security updates and latest versions installed. Including allowing new Kubernetes versions to be rolled out to
    # nodes automatically. Defaults to 720h if not defined.
    # Note: Drift detection will also force recreation of nodes when a new version exist, when detected.
    expireAfter: 336h # 14 days = 14 * 24 Hours

    # The amount of time that a node can be draining before it's forcibly deleted. A node begins draining when a delete call is made against it, starting
    # its finalization flow. Pods with TerminationGracePeriodSeconds will be deleted preemptively before this terminationGracePeriod ends to give as much time to cleanup as possible.
    # If your pod's terminationGracePeriodSeconds is larger than this terminationGracePeriod, Karpenter may forcibly delete the pod
    # before it has its full terminationGracePeriod to cleanup.
    # Note: changing this value in the nodepool will drift the nodeclaims.
    terminationGracePeriod: 48h

    # Priority given to the NodePool when the scheduler considers which NodePool
    # to select. Higher weights indicate higher priority when comparing NodePools.
    # Specifying no weight is equivalent to specifying a weight of 0.
    weight: 10

    # Defines how much resources this provisioner is allowed to launch
    limits:
      cpu: 1000
      memory: 1000Gi

    # Specify requirements constraints
    requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
      - key: kubernetes.io/os
        operator: In
        values: ["linux"]
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand", "spot"]
      - key: karpenter.k8s.aws/instance-category
        operator: In
        values: ["c", "m", "r"]
      - key: karpenter.k8s.aws/instance-generation
        operator: Gt
        values: ["2"]

  onDemand:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 2m
    expireAfter: 336h # 14 days = 14 * 24 Hours
    terminationGracePeriod: 48h
    weight: 2
    limits:
      cpu: 100
      memory: 1000Gi

    #taints:
    #  - key: karpenter.sh/nodepool
    #    effect: NoSchedule

    requirements:
      - key: kubernetes.io/arch
        operator: In
        values: [ "amd64" ]
      - key: kubernetes.io/os
        operator: In
        values: [ "linux" ]
      - key: karpenter.sh/capacity-type
        operator: In
        values: [ "on-demand" ]
      - key: karpenter.k8s.aws/instance-category
        operator: In
        values: [ "c", "m", "r" ]
      - key: karpenter.k8s.aws/instance-generation
        operator: Gt
        values: [ "2" ]

  unmanaged:
    consolidationPolicy: WhenEmpty
    consolidateAfter: Never
    expireAfter: 2160h # 3 months expire, expects to be managed by something else (before the 3 month expire).
    terminationGracePeriod: 48h
    #budgets:
    #  - nodes: "1"
    #  - nodes: "0"
    #    schedule: "@daily" # Set a schedule for when to block disruptions
    #    duration: 10m      # Block disruptions for the specified duration from the start of the schedule
    weight: 1
    limits:
      cpu: 50
      memory: 250Gi

    #taints:
    #  - key: karpenter.sh/nodepool
    #    effect: NoSchedule

    requirements:
      - key: kubernetes.io/arch
        operator: In
        values: [ "amd64" ]
      - key: kubernetes.io/os
        operator: In
        values: [ "linux" ]
      - key: karpenter.sh/capacity-type
        operator: In
        values: [ "on-demand" ]
      - key: karpenter.k8s.aws/instance-category
        operator: In
        values: [ "c", "m", "r" ]
      - key: karpenter.k8s.aws/instance-generation
        operator: Gt
        values: [ "2" ]

  reservedInstances:
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 2m
    expireAfter: 336h # 14 days = 14 * 24 Hours
    terminationGracePeriod: 48h
    weight: 50
    limits:
      cpu: 0
      memory: 1000Gi

    requirements:
      - key: kubernetes.io/arch
        operator: In
        values: [ "amd64" ]
      - key: kubernetes.io/os
        operator: In
        values: [ "linux" ]
      - key: karpenter.sh/capacity-type
        operator: In
        values: [ "on-demand" ]
      - key: karpenter.k8s.aws/instance-category
        operator: In
        values: [ "c", "m", "r" ]
      - key: karpenter.k8s.aws/instance-generation
        operator: Gt
        values: [ "2" ]
